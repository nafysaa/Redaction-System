{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn01ufc2jEBA",
        "outputId": "6a723cf7-2f5f-4057-ab75-82b46b98598b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFQnvsVMkjdq",
        "outputId": "79699b6a-214f-4721-ffbf-c521db3229ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-21 16:29:58.578167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python -m spacy init config base_config.cfg --lang en --pipeline ner\n",
        "python -m spacy train base_config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy --gpu-id 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ifhUgVIkl2r",
        "outputId": "6a00903e-cca4-4739-c888-07dd1a72eb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-21 16:30:17.957361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
            "install the spacy-transformers package and re-run this command. The config\n",
            "generated now does not use transformers.\u001b[0m\n",
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: en\n",
            "- Pipeline: ner\n",
            "- Optimize for: efficiency\n",
            "- Hardware: CPU\n",
            "- Transformer: None\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "base_config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train base_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
            "2023-09-21 16:30:29.669567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/cli/_util.py\", line 92, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 778, in main\n",
            "    return _main(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 216, in _main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 683, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/cli/train.py\", line 54, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/cli/train.py\", line 76, in train\n",
            "    setup_gpu(use_gpu)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/cli/_util.py\", line 599, in setup_gpu\n",
            "    require_gpu(use_gpu)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/util.py\", line 230, in require_gpu\n",
            "    raise ValueError(\"Cannot use GPU, CuPy is not installed\")\n",
            "ValueError: Cannot use GPU, CuPy is not installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load spaCy model with NER component\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read your dataset (assuming it's in a CSV file with one sentence per line)\n",
        "dataset_file = \"redaction_train_set.csv\"\n",
        "with open(dataset_file, \"r\") as file:\n",
        "    sentences = file.readlines()\n",
        "\n",
        "# Initialize a list to store annotated data\n",
        "annotated_data = []\n",
        "\n",
        "# Process and annotate each sentence\n",
        "for sentence in sentences:\n",
        "    sentence = sentence.strip()  # Remove leading/trailing whitespace\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Use displacy to visualize and annotate named entities\n",
        "    spacy.displacy.render(doc, style=\"ent\", jupyter=True)\n",
        "\n",
        "    # Manually annotate the entities using the visualizer.\n",
        "    # You can select and label the entities in the visualized sentence.\n",
        "\n",
        "    # After annotating, ask the user to input the entities in the format:\n",
        "    # [(start_char, end_char, label), ...]\n",
        "    annotations = input(\"Enter the entities (start, end, label) or 'q' to quit: \")\n",
        "\n",
        "    if annotations.lower() == 'q':\n",
        "        break\n",
        "\n",
        "    # Parse the entered annotations\n",
        "    entities = [eval(annotation) for annotation in annotations.split(\",\")]\n",
        "\n",
        "    # Create a dictionary with \"entities\" key for annotation\n",
        "    annotations_dict = {\"entities\": entities}\n",
        "\n",
        "    # Update the Doc with the manually annotated entities\n",
        "    doc.ents = []\n",
        "    for start, end, label in entities:\n",
        "        span = doc.char_span(start, end, label=label)\n",
        "        if span is not None:\n",
        "            doc.ents = list(doc.ents) + [span]\n",
        "\n",
        "    # Append the annotated sentence to the list\n",
        "    annotated_data.append((sentence, annotations_dict))\n",
        "\n",
        "# Save the annotations to a JSONL file\n",
        "output_file = \"annotated_data.jsonl\"\n",
        "with open(output_file, \"w\") as file:\n",
        "    for sentence, annotations in annotated_data:\n",
        "        record = {\n",
        "            \"text\": sentence,\n",
        "            \"annotations\": annotations\n",
        "        }\n",
        "        file.write(json.dumps(record) + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "bEsOPnLYk9qe",
        "outputId": "2db94112-4081-41ac-b699-0ea4f733771f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">&quot;Hi, my name is \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Emma Thompson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", and I have forgotten the password for my email account. It's emmathompson95@gmail.com.&quot;</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the entities (start, end, label) or 'q' to quit: 14,26,PERSON\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0fdc43451bc3>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Parse the entered annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mannotation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Create a dictionary with \"entities\" key for annotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0fdc43451bc3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Parse the entered annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mannotation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Create a dictionary with \"entities\" key for annotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PERSON' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import csv\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read your dataset, assuming it's in a CSV file with one sentence per row\n",
        "with open('redaction_train_set.csv', 'r') as csv_file:\n",
        "    reader = csv.reader(csv_file)\n",
        "    for row in reader:\n",
        "        sentence = row[0]  # Assuming the sentence is in the first column\n",
        "\n",
        "        # Process the sentence with spaCy\n",
        "        doc = nlp(sentence)\n",
        "\n",
        "        # Extract named entities and their labels\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "        # Print or store the recognized entities and labels for each sentence\n",
        "        print(entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL41hK17mdg2",
        "outputId": "10750ebe-3ce3-4656-d4bb-1298d57592f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Emma Thompson', 'PERSON')]\n",
            "[('Alex Johnson', 'PERSON'), ('123', 'CARDINAL'), ('Apple Street', 'ORG'), ('New York City', 'GPE'), ('10001', 'DATE')]\n",
            "[('Chris Evans', 'PERSON'), ('569741285', 'DATE')]\n",
            "[('morning', 'TIME'), ('Emily Roberts', 'PERSON'), ('January next year', 'DATE')]\n",
            "[('James Davis', 'PERSON'), ('90210', 'DATE')]\n",
            "[('Sophia Adams', 'PERSON'), ('June 15th, 1990', 'DATE')]\n",
            "[('afternoon', 'TIME'), ('Olivia Wilson', 'PERSON'), ('555-123-4567', 'CARDINAL'), ('555', 'CARDINAL')]\n",
            "[('Liam Martinez', 'PERSON'), ('July 5th', 'DATE'), ('0612', 'DATE')]\n",
            "[('Mia Wilson', 'PERSON')]\n",
            "[('Noah Jackson', 'PERSON')]\n",
            "[('afternoon', 'TIME'), ('Ava Anderson', 'PERSON'), ('avaanderson20', 'LANGUAGE')]\n",
            "[('Ethan Murphy', 'PERSON'), ('555', 'CARDINAL')]\n",
            "[('Isabella Carter', 'PERSON'), ('9876543210987654', 'DATE')]\n",
            "[('Benjamin Davis', 'PERSON'), ('321', 'CARDINAL'), ('Los Angeles', 'GPE')]\n",
            "[('Mia Johnson', 'PERSON'), ('Samsung Galaxy', 'ORG'), ('IMEI', 'PRODUCT'), ('123456789012345', 'DATE')]\n",
            "[('morning', 'TIME'), ('Olivia Turner', 'ORG')]\n",
            "[('William Anderson', 'PERSON'), ('456 Elm Lane', 'FAC'), ('Chicago', 'GPE')]\n",
            "[('Sophia White', 'PERSON'), ('monthly', 'DATE'), ('0987654321', 'DATE')]\n",
            "[('Liam Lee', 'PERSON')]\n",
            "[('Mia Thomas', 'PERSON'), ('1357', 'DATE')]\n",
            "[('afternoon', 'TIME'), ('James Clark', 'PERSON'), ('7890', 'DATE')]\n",
            "[('Olivia Harris', 'PERSON'), ('789 Maple Avenue', 'ORG'), ('San Francisco', 'GPE'), ('CA 94102', 'ORG')]\n",
            "[('Noah Walker', 'PERSON')]\n",
            "[('Ava Rodriguez', 'PERSON'), ('567', 'CARDINAL'), ('Oak Street', 'GPE'), ('Miami', 'GPE'), ('FL', 'ORG'), ('33101', 'DATE')]\n",
            "[('Benjamin Lewis', 'PERSON')]\n",
            "[('morning', 'TIME'), ('Isabella Mitchell', 'PERSON'), ('izzy24', 'LOC')]\n",
            "[('William Perez', 'PERSON'), ('PIN', 'ORG'), ('four', 'CARDINAL'), ('1234', 'DATE'), ('9876', 'DATE')]\n",
            "[('Mia Rivera', 'PERSON')]\n",
            "[('Benjamin Scott', 'PERSON'), ('555', 'CARDINAL'), ('555', 'CARDINAL')]\n",
            "[('Olivia Ward', 'GPE')]\n",
            "[('afternoon', 'TIME'), ('Noah Davis', 'PERSON'), ('4578', 'DATE')]\n",
            "[('Ava Martin', 'PERSON'), ('AC1234567', 'PRODUCT')]\n",
            "[('Liam Adams', 'PERSON'), ('1839', 'DATE')]\n",
            "[('Mia Turner', 'PERSON'), ('mia_turner1958', 'DATE')]\n",
            "[('Noah Campbell', 'PERSON')]\n",
            "[('Ariana Ward', 'PERSON'), ('789 Pine Street', 'ORG'), ('Seattle', 'GPE')]\n",
            "[('Jackson Thompson', 'PERSON'), ('555', 'CARDINAL')]\n",
            "[('Scarlett Parker', 'PERSON')]\n",
            "[('Benjamin Green', 'PERSON'), ('598751324', 'DATE')]\n",
            "[('afternoon', 'TIME'), ('Emily Davis', 'PERSON')]\n",
            "[('Oliver Wilson', 'PERSON')]\n",
            "[('morning', 'TIME'), ('Sophia Martinez', 'PERSON'), ('7890123456789012', 'DATE'), ('November next year', 'DATE')]\n",
            "[('James Ramirez', 'PERSON'), ('10022', 'DATE')]\n",
            "[('Mia Flores', 'PERSON'), ('six', 'CARDINAL'), ('123456', 'DATE'), ('987654', 'DATE')]\n",
            "[('Olivia Phillips', 'ORG'), ('AB987654', 'ORG')]\n",
            "[('Noah Lopez', 'PERSON'), ('555', 'CARDINAL'), ('555', 'CARDINAL')]\n",
            "[('afternoon', 'TIME'), ('Ava King', 'PERSON'), ('June 10th', 'DATE'), ('1234', 'DATE')]\n",
            "[('Benjamin Alexander', 'PERSON')]\n",
            "[('Mia Watson', 'PERSON'), ('321', 'CARDINAL'), ('Oak Drive', 'GPE'), ('San Diego', 'GPE'), ('CA 92101', 'PERSON')]\n",
            "[('Oliver Brooks', 'PERSON')]\n",
            "[('Sophia Evans', 'PERSON'), ('PIN', 'ORG'), ('four', 'CARDINAL'), ('5', 'DATE')]\n",
            "[('morning', 'TIME'), ('James Rodriguez', 'PERSON'), ('james.rodriguez2020@gmail.com', 'PERSON')]\n",
            "[('Emily Lewis', 'PERSON'), ('123', 'CARDINAL'), ('Maple Street', 'FAC'), ('456 Elm Avenue', 'FAC')]\n",
            "[('Liam Adams', 'PERSON')]\n",
            "[('Mia Gray', 'PERSON')]\n",
            "[('Oliver Martinez', 'PERSON'), ('NY9876543210', 'ORG')]\n",
            "[('afternoon', 'TIME'), ('Sophia Turner', 'PERSON'), ('2468', 'DATE')]\n",
            "[('James Thompson', 'PERSON'), ('123 Pine Street', 'FAC')]\n",
            "[('Emily Murphy', 'PERSON')]\n",
            "[('Olivia Robinson', 'PERSON')]\n",
            "[('John Smith', 'PERSON')]\n",
            "[('Amy Johnson', 'PERSON'), ('Los Angeles', 'GPE'), ('DOB', 'ORG'), ('04/25/1988', 'DATE')]\n",
            "[('David Brown', 'PERSON'), ('60619', 'DATE')]\n",
            "[('Emily Davis', 'PERSON'), ('Houston', 'GPE')]\n",
            "[('Michael Wilson', 'PERSON'), ('46381906530248', 'DATE')]\n",
            "[('Jennifer Rodriguez', 'PERSON')]\n",
            "[('Alex Edwards', 'PERSON'), ('DOB - 10/15/1992', 'ORG')]\n",
            "[('Emma Smith', 'PERSON'), ('Seattle', 'GPE')]\n",
            "[('Daniel Johnson', 'PERSON'), ('San Francisco', 'GPE'), ('94109', 'DATE')]\n",
            "[('Jane Williams', 'PERSON')]\n",
            "[('Ryan Thompson', 'PERSON')]\n",
            "[('Olivia Miller', 'PERSON'), ('ATM', 'ORG'), ('1234', 'DATE')]\n",
            "[('Paul Davis', 'PERSON'), ('9074', 'DATE')]\n",
            "[('Sophia Robinson', 'PERSON'), ('SSN', 'ORG')]\n",
            "[('Matthew Garcia', 'PERSON'), ('9876', 'DATE')]\n",
            "[('morning', 'TIME'), ('Vanessa Green', 'PERSON')]\n",
            "[('Aaron Turner', 'PERSON'), ('Miami', 'GPE'), ('6578', 'DATE')]\n",
            "[('Isabelle Mitchell', 'PERSON')]\n",
            "[('Andrew Taylor', 'PERSON'), ('number - 145', 'CARDINAL')]\n",
            "[('Abigail Martinez', 'PERSON'), ('Phoenix', 'GPE')]\n",
            "[]\n",
            "[('Chloe Wood', 'ORG')]\n",
            "[('afternoon', 'TIME'), ('Dylan Cook', 'PERSON'), ('four', 'CARDINAL')]\n",
            "[('Amelia Price', 'PERSON')]\n",
            "[('503', 'CARDINAL'), ('555-1234', 'DATE')]\n",
            "[('afternoon', 'TIME'), ('876-345-1234', 'CARDINAL')]\n",
            "[('555', 'CARDINAL')]\n",
            "[('814', 'CARDINAL'), ('276', 'CARDINAL')]\n",
            "[('345-721-9087', 'CARDINAL')]\n",
            "[('918', 'CARDINAL'), ('555-8765', 'DATE')]\n",
            "[('347-643-9021', 'CARDINAL')]\n",
            "[('5551237890', 'DATE')]\n",
            "[('morning', 'TIME'), ('281-576-9302', 'CARDINAL')]\n",
            "[('456-837-1293', 'CARDINAL')]\n",
            "[('345', 'CARDINAL'), ('678', 'CARDINAL')]\n",
            "[('afternoon', 'TIME'), ('279-486-2357', 'CARDINAL')]\n",
            "[('9235042385', 'DATE')]\n",
            "[('5733219876', 'DATE')]\n",
            "[('718', 'CARDINAL'), ('349', 'CARDINAL')]\n",
            "[('324', 'CARDINAL')]\n",
            "[('987-654-3120', 'CARDINAL')]\n",
            "[('536', 'CARDINAL'), ('942-0182', 'CARDINAL')]\n",
            "[('morning', 'TIME'), ('294', 'CARDINAL')]\n",
            "[('976', 'CARDINAL')]\n",
            "[('653', 'CARDINAL'), ('432-0987', 'DATE')]\n",
            "[('afternoon', 'TIME'), ('328', 'CARDINAL')]\n",
            "[('3795046325', 'DATE')]\n",
            "[('5089432720', 'DATE')]\n",
            "[('245', 'CARDINAL'), ('983', 'CARDINAL')]\n",
            "[('7254386902', 'DATE')]\n",
            "[('960-783-4521', 'CARDINAL')]\n",
            "[('758', 'CARDINAL'), ('304', 'CARDINAL')]\n",
            "[('8093572461', 'DATE')]\n",
            "[('279-215-3450', 'CARDINAL')]\n",
            "[('morning', 'TIME'), ('740-420-6729', 'CARDINAL')]\n",
            "[('5028743971', 'DATE')]\n",
            "[('503', 'CARDINAL')]\n",
            "[('Emma Thompson', 'PERSON')]\n",
            "[('Alex Johnson', 'PERSON'), ('123', 'CARDINAL'), ('Main Street', 'FAC'), ('Los Angeles', 'GPE'), ('90001', 'DATE')]\n",
            "[('Sarah Anderson', 'PERSON')]\n",
            "[('James Smith', 'PERSON'), ('May 15th', 'DATE'), ('1985', 'DATE')]\n",
            "[('Olivia Hernandez', 'PERSON')]\n",
            "[('Jacob Lee', 'PERSON')]\n",
            "[('Mia Davis', 'PERSON')]\n",
            "[('Ethan Wilson', 'PERSON')]\n",
            "[('Isabella Garcia', 'PERSON')]\n",
            "[('Liam Martin', 'PERSON')]\n",
            "[('Sophia Thompson', 'PERSON')]\n",
            "[('Benjamin Wright', 'PERSON'), ('456 Oakwood Drive', 'FAC'), ('Chicago', 'GPE'), ('60616', 'DATE')]\n",
            "[('Emily Martinez', 'PERSON')]\n",
            "[('Michael Robinson', 'PERSON'), ('three', 'CARDINAL'), ('the past month', 'DATE')]\n",
            "[('Ava Lewis', 'FAC')]\n",
            "[('Elijah Adams', 'PERSON')]\n",
            "[('Harper Turner', 'PERSON')]\n",
            "[('Scarlett Rodriguez', 'PERSON')]\n",
            "[('Jackson Morgan', 'PERSON')]\n",
            "[('Madison Bell', 'ORG')]\n",
            "[('Aiden Murphy', 'PERSON')]\n",
            "[('Zoe Alexander', 'PERSON'), ('ATM', 'ORG')]\n",
            "[('Caleb King', 'PERSON')]\n",
            "[('Scarlett Reed', 'PERSON'), ('one', 'CARDINAL')]\n",
            "[('Savannah Mitchell', 'PERSON')]\n",
            "[('Christopher Wright', 'PERSON')]\n",
            "[('Victoria Diaz', 'PERSON')]\n",
            "[('Grayson Rivera', 'FAC')]\n",
            "[('Dylan Davis', 'PERSON')]\n",
            "[('Leah Foster', 'PERSON')]\n",
            "[('Mateo Coleman', 'PERSON')]\n",
            "[('Grace Sanders', 'PERSON')]\n",
            "[('Jack Barnes', 'PERSON'), ('789 Elmwood Road', 'ORG'), ('New York', 'GPE'), ('10010', 'DATE')]\n",
            "[('Lily Phillips', 'PERSON')]\n",
            "[('Luke Perry', 'PERSON')]\n",
            "[('Layla Richardson', 'PERSON')]\n",
            "[('Aaron Peterson', 'PERSON')]\n",
            "[('Ellie Edwards', 'PERSON')]\n",
            "[('Nicholas Brooks', 'PERSON')]\n",
            "[('Chloe Turner', 'PERSON')]\n",
            "[('Gabriel Hayes', 'PERSON')]\n",
            "[('Maya Simmons', 'PERSON')]\n",
            "[('Henry Evans', 'PERSON')]\n",
            "[('Peyton Griffin', 'GPE')]\n",
            "[('Elena Bell', 'ORG')]\n",
            "[('Oliver Campbell', 'PERSON')]\n",
            "[('Lucy Collins', 'PERSON')]\n",
            "[('Wyatt Rivera', 'ORG')]\n",
            "[('Penelope Watson', 'FAC')]\n",
            "[('Isaiah Hughes', 'PERSON')]\n",
            "[('456', 'CARDINAL'), ('789-1234', 'DATE')]\n",
            "[('890', 'CARDINAL')]\n",
            "[('3459872315', 'DATE')]\n",
            "[('4243671398', 'DATE')]\n",
            "[('4398762153', 'DATE')]\n",
            "[('8713298654', 'DATE')]\n",
            "[('823)180-4951', 'DATE')]\n",
            "[('3405628974', 'DATE')]\n",
            "[]\n",
            "[('8129876543', 'DATE')]\n",
            "[('3284910726', 'DATE')]\n",
            "[('3142789056', 'CARDINAL')]\n",
            "[('2917634058', 'DATE')]\n",
            "[('8723491783', 'DATE')]\n",
            "[('9028645132', 'DATE')]\n",
            "[('743)501-2498', 'DATE')]\n",
            "[('3946816570', 'DATE')]\n",
            "[('7618235094', 'DATE')]\n",
            "[]\n",
            "[('2038956714', 'DATE')]\n",
            "[('917', 'CARDINAL')]\n",
            "[('123', 'CARDINAL')]\n",
            "[('555-123-4567', 'CARDINAL')]\n",
            "[('987-654-3210', 'CARDINAL')]\n",
            "[('8056324891', 'DATE')]\n",
            "[('213)987-6543', 'CARDINAL')]\n",
            "[('408', 'CARDINAL')]\n",
            "[('9493685741', 'DATE')]\n",
            "[]\n",
            "[('7024879654', 'DATE')]\n",
            "[('9183409502', 'DATE')]\n",
            "[('4809023847', 'CARDINAL')]\n",
            "[('6282047863', 'DATE')]\n",
            "[('6318957048', 'CARDINAL')]\n",
            "[]\n",
            "[('619)482-9370', 'DATE')]\n",
            "[('8721156953', 'DATE')]\n",
            "[('2089503176', 'DATE')]\n",
            "[('two', 'CARDINAL'), ('407)307-8964', 'DATE')]\n",
            "[('5734291876', 'DATE')]\n",
            "[('987', 'CARDINAL'), ('654', 'CARDINAL')]\n",
            "[('456-789-1234', 'CARDINAL')]\n",
            "[('890-123-4567', 'CARDINAL')]\n",
            "[('234-567-8901', 'CARDINAL')]\n",
            "[('5673218901', 'DATE')]\n",
            "[('321', 'CARDINAL'), ('890', 'CARDINAL')]\n",
            "[('678-901-2345', 'CARDINAL')]\n",
            "[('098-765-4321', 'CARDINAL')]\n",
            "[('753', 'CARDINAL'), ('520-4918', 'CARDINAL')]\n",
            "[('6132948576', 'DATE')]\n",
            "[('3591867420', 'DATE')]\n",
            "[('6589310742', 'CARDINAL')]\n",
            "[]\n",
            "[('2765439081', 'DATE')]\n",
            "[('6472859034', 'DATE')]\n",
            "[]\n",
            "[('9352841076', 'DATE')]\n",
            "[('2379461085', 'DATE')]\n",
            "[('two', 'CARDINAL'), ('259)683-7514', 'DATE')]\n",
            "[('6820453912', 'DATE')]\n",
            "[('876', 'CARDINAL'), ('543-2109', 'DATE')]\n",
            "[('543', 'CARDINAL')]\n",
            "[('901', 'CARDINAL')]\n",
            "[('234-567-8901', 'CARDINAL')]\n",
            "[('5670983214', 'DATE')]\n",
            "[('765', 'CARDINAL'), ('890-1234', 'DATE')]\n",
            "[('901', 'CARDINAL')]\n",
            "[('890-123-4567', 'CARDINAL')]\n",
            "[('214', 'CARDINAL')]\n",
            "[]\n",
            "[]\n",
            "[('September 14, 1985', 'DATE')]\n",
            "[('90210', 'DATE')]\n",
            "[('7654321098', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('AB1234567', 'PRODUCT')]\n",
            "[]\n",
            "[]\n",
            "[('5678904321', 'DATE')]\n",
            "[('October 26, 1990', 'DATE')]\n",
            "[('60601', 'CARDINAL')]\n",
            "[]\n",
            "[]\n",
            "[('AC9876543', 'PRODUCT')]\n",
            "[('the Personal Identifying Number', 'ORG')]\n",
            "[('janedoe789@zoho.com', 'ORG')]\n",
            "[('5432109876', 'DATE')]\n",
            "[('May 30, 1987', 'DATE')]\n",
            "[('94536', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('XY1234567', 'PRODUCT')]\n",
            "[]\n",
            "[('james.smith15@outlook.com', 'PERSON')]\n",
            "[('9876543210', 'DATE')]\n",
            "[('July 4, 1992', 'DATE')]\n",
            "[('94107', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('JK9876543', 'PRODUCT')]\n",
            "[('the Personal Identifying Number', 'ORG')]\n",
            "[]\n",
            "[('1234509876', 'DATE')]\n",
            "[('January 1, 1995', 'DATE')]\n",
            "[('94110', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('LM1234567', 'PRODUCT')]\n",
            "[]\n",
            "[]\n",
            "[('6543210987', 'DATE')]\n",
            "[('November 20, 1988', 'DATE')]\n",
            "[('90210', 'CARDINAL')]\n",
            "[]\n",
            "[]\n",
            "[('PQ9876543', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('5432109876', 'DATE')]\n",
            "[('August 15, 1993', 'DATE')]\n",
            "[('60610', 'CARDINAL')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('9876543210', 'CARDINAL')]\n",
            "[('February 10, 1997', 'DATE')]\n",
            "[('90212', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('one', 'CARDINAL'), ('TU9876543', 'PRODUCT')]\n",
            "[]\n",
            "[]\n",
            "[('1234567890', 'DATE')]\n",
            "[('April 5, 1989', 'DATE')]\n",
            "[('60611', 'CARDINAL')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('5678901234', 'DATE')]\n",
            "[('March 1, 1991', 'DATE')]\n",
            "[('98101', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('9876543210', 'CARDINAL')]\n",
            "[('August 18, 1994', 'DATE')]\n",
            "[('94607', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('one', 'CARDINAL'), ('YZ1234567', 'PRODUCT')]\n",
            "[]\n",
            "[]\n",
            "[('4567890123', 'DATE')]\n",
            "[('January 24, 1996', 'DATE')]\n",
            "[('12345', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('AB1234567', 'PRODUCT')]\n",
            "[]\n",
            "[]\n",
            "[('2345678901', 'DATE')]\n",
            "[('November 12, 1999', 'DATE')]\n",
            "[('90215', 'DATE')]\n",
            "[]\n",
            "[('July 15th, 1990', 'DATE')]\n",
            "[('12345', 'DATE')]\n",
            "[('9876543210', 'CARDINAL')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('123-4567', 'CARDINAL'), ('860', 'CARDINAL')]\n",
            "[]\n",
            "[('December 30th, 1985', 'DATE'), ('November 2nd, 1992', 'DATE')]\n",
            "[('54321', 'DATE')]\n",
            "[('5489576532', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('415', 'CARDINAL'), ('444', 'CARDINAL'), ('415', 'CARDINAL')]\n",
            "[]\n",
            "[('February 3rd, 1993', 'DATE'), ('August 19th, 1990', 'DATE')]\n",
            "[('98765', 'PERSON')]\n",
            "[('2345678901', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('720', 'CARDINAL'), ('555-1234', 'DATE')]\n",
            "[]\n",
            "[('July 26th, 1988', 'DATE')]\n",
            "[('54321', 'DATE')]\n",
            "[('8567145382', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('615', 'CARDINAL'), ('123-4567', 'CARDINAL'), ('615', 'CARDINAL')]\n",
            "[]\n",
            "[('September 30th, 1992', 'DATE'), ('July 4th, 1985', 'DATE')]\n",
            "[('98765', 'PERSON')]\n",
            "[('8639217405', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('W90315Could', 'PERSON')]\n",
            "[('530', 'CARDINAL'), ('555', 'CARDINAL')]\n",
            "[]\n",
            "[('August 15th, 1982', 'DATE'), ('February 20th, 1990', 'DATE')]\n",
            "[('54321', 'DATE')]\n",
            "[('6548291735', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('925', 'CARDINAL'), ('555', 'CARDINAL')]\n",
            "[]\n",
            "[('July 10th,', 'DATE'), ('1980', 'DATE'), ('September 20th, 1993', 'DATE')]\n",
            "[]\n",
            "[('5th', 'ORDINAL'), ('August 1990', 'DATE')]\n",
            "[('90210', 'DATE'), ('10001', 'DATE')]\n",
            "[('XXXX-XXXX-XXXX-1234', 'PERSON')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('PIN', 'ORG'), ('PIN', 'ORG'), ('123456', 'DATE')]\n",
            "[('415', 'CARDINAL'), ('567-1234', 'DATE'), ('646', 'CARDINAL'), ('789', 'CARDINAL')]\n",
            "[('jane.doe@yahoo.com', 'ORG')]\n",
            "[('60601 to 75201', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('PIN', 'ORG')]\n",
            "[]\n",
            "[]\n",
            "[('the month of November 20My', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('28th June 1985', 'DATE'), ('2021', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('Bank', 'ORG')]\n",
            "[('1988', 'DATE')]\n",
            "[('123 Main Street', 'FAC'), ('Chicago', 'GPE'), ('Illinois', 'GPE')]\n",
            "[('XXXX-XXXX-XXXX-5678', 'ORG')]\n",
            "[]\n",
            "[('abcd1234', 'PERSON')]\n",
            "[('PIN', 'ORG')]\n",
            "[]\n",
            "[('the month of December 2021', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('1st January 2000', 'DATE'), ('15th June 1989', 'DATE')]\n",
            "[('12345', 'DATE'), ('54321', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('210', 'CARDINAL'), ('987-6543', 'CARDINAL')]\n",
            "[('Personal Identifying Number (', 'WORK_OF_ART'), ('PIN', 'ORG')]\n",
            "[]\n",
            "[('next month', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('Assistance', 'PERSON'), ('92101', 'DATE')]\n",
            "[('4321', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('March 15, 1985', 'DATE')]\n",
            "[('12345', 'DATE')]\n",
            "[('987654321', 'DATE')]\n",
            "[('246813', 'DATE')]\n",
            "[('AB1234567', 'PRODUCT')]\n",
            "[]\n",
            "[('September 10, 1990', 'DATE')]\n",
            "[('54321', 'DATE')]\n",
            "[('654321987', 'DATE')]\n",
            "[('135792', 'DATE')]\n",
            "[('CD9876543', 'PRODUCT')]\n",
            "[]\n",
            "[('May 20, 1988', 'DATE')]\n",
            "[('98765', 'PERSON')]\n",
            "[('123456789', 'DATE')]\n",
            "[('987654', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('July 25, 1995', 'DATE')]\n",
            "[('56789', 'DATE')]\n",
            "[('789654321', 'DATE')]\n",
            "[('369852', 'DATE')]\n",
            "[('GH8765432', 'PRODUCT')]\n",
            "[]\n",
            "[('March 15, 1985', 'DATE')]\n",
            "[('12345', 'DATE')]\n",
            "[('987654321', 'DATE')]\n",
            "[('246813', 'DATE')]\n",
            "[('AB1234567', 'PRODUCT')]\n",
            "[]\n",
            "[('September 10, 1990', 'DATE')]\n",
            "[('54321', 'DATE')]\n",
            "[('654321987', 'DATE')]\n",
            "[('135792', 'DATE')]\n",
            "[('CD9876543', 'PRODUCT')]\n",
            "[]\n",
            "[('May 20, 1988', 'DATE')]\n",
            "[('98765', 'PERSON')]\n",
            "[('123456789', 'DATE')]\n",
            "[('987654', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('July 25, 1995', 'DATE')]\n",
            "[('56789', 'DATE')]\n",
            "[('789654321', 'DATE')]\n",
            "[('369852', 'DATE')]\n",
            "[('GH8765432', 'PRODUCT')]\n",
            "[]\n",
            "[('March 15, 1985', 'DATE')]\n",
            "[('12345', 'DATE')]\n",
            "[('987654321', 'DATE')]\n",
            "[('246813', 'DATE')]\n",
            "[('AB1234567', 'PRODUCT')]\n",
            "[]\n",
            "[('September 10, 1990', 'DATE')]\n",
            "[('54321', 'DATE')]\n",
            "[('654321987', 'DATE')]\n",
            "[('135792', 'DATE')]\n",
            "[('CD9876543', 'PRODUCT')]\n",
            "[]\n",
            "[('May 20, 1988', 'DATE')]\n",
            "[('98765', 'PERSON')]\n",
            "[('123456789', 'DATE')]\n",
            "[('987654', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('July 25, 1995', 'DATE')]\n",
            "[('56789', 'DATE')]\n",
            "[('789654321', 'DATE')]\n",
            "[('369852', 'DATE')]\n",
            "[('GH8765432', 'PRODUCT')]\n",
            "[]\n",
            "[('March 15, 1985', 'DATE')]\n",
            "[('12345', 'DATE')]\n",
            "[('987654321', 'DATE')]\n",
            "[('246813', 'DATE')]\n",
            "[('AB1234567', 'PRODUCT')]\n",
            "[]\n",
            "[('September 10, 1990', 'DATE')]\n",
            "[('54321', 'DATE')]\n",
            "[('654321987', 'DATE')]\n",
            "[('135792', 'DATE')]\n",
            "[('CD9876543', 'PRODUCT')]\n",
            "[]\n",
            "[('May 20, 1988', 'DATE')]\n",
            "[('98765', 'PERSON')]\n",
            "[('123456789', 'DATE')]\n",
            "[('987654', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('July 25, 1995', 'DATE')]\n",
            "[('56789', 'DATE')]\n",
            "[('789654321', 'DATE')]\n",
            "[('369852', 'DATE')]\n",
            "[('GH8765432', 'PRODUCT')]\n",
            "[]\n",
            "[('March 15, 1985', 'DATE')]\n",
            "[('12345', 'DATE')]\n",
            "[('987654321', 'DATE')]\n",
            "[]\n",
            "[('March 15, 1985', 'DATE')]\n",
            "[('12345', 'DATE')]\n",
            "[('987654321', 'DATE')]\n",
            "[('246813', 'DATE')]\n",
            "[('AB1234567', 'PRODUCT')]\n",
            "[]\n",
            "[('September 10, 1990', 'DATE')]\n",
            "[('54321', 'DATE')]\n",
            "[('654321987', 'DATE')]\n",
            "[('135792', 'DATE')]\n",
            "[('CD9876543', 'PRODUCT')]\n",
            "[]\n",
            "[('May 20, 1988', 'DATE')]\n",
            "[('98765', 'PERSON')]\n",
            "[('123456789', 'DATE')]\n",
            "[('987654', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('July 25, 1995', 'DATE')]\n",
            "[('56789', 'DATE')]\n",
            "[('789654321', 'DATE')]\n",
            "[('369852', 'DATE')]\n",
            "[('GH8765432', 'PRODUCT')]\n",
            "[]\n",
            "[('March 15, 1985', 'DATE')]\n",
            "[('12345', 'DATE')]\n",
            "[('987654321', 'DATE')]\n",
            "[('246813', 'DATE')]\n",
            "[('AB1234567', 'PRODUCT')]\n",
            "[]\n",
            "[('September 10, 1990', 'DATE')]\n",
            "[('54321', 'DATE')]\n",
            "[('654321987', 'DATE')]\n",
            "[('135792', 'DATE')]\n",
            "[('CD9876543', 'PRODUCT')]\n",
            "[]\n",
            "[('May 20, 1988', 'DATE')]\n",
            "[('98765', 'PERSON')]\n",
            "[('123456789', 'DATE')]\n",
            "[('987654', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('July 25, 1995', 'DATE')]\n",
            "[('56789', 'DATE')]\n",
            "[('789654321', 'DATE')]\n",
            "[('369852', 'DATE')]\n",
            "[('GH8765432', 'PRODUCT')]\n",
            "[]\n",
            "[('March 15, 1985', 'DATE')]\n",
            "[('12345', 'DATE')]\n",
            "[('987654321', 'DATE')]\n",
            "[('246813', 'DATE')]\n",
            "[('AB1234567', 'PRODUCT')]\n",
            "[]\n",
            "[('September 10, 1990', 'DATE')]\n",
            "[('54321', 'DATE')]\n",
            "[('654321987', 'DATE')]\n",
            "[('135792', 'DATE')]\n",
            "[('CD9876543', 'PRODUCT')]\n",
            "[]\n",
            "[('May 20, 1988', 'DATE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import csv\n",
        "import json\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize a list to store training examples\n",
        "training_data = []\n",
        "\n",
        "# Read your dataset, assuming it's in a CSV file with one sentence per row\n",
        "with open('redaction_train_set.csv', 'r') as csv_file:\n",
        "    reader = csv.reader(csv_file)\n",
        "    for row in reader:\n",
        "        sentence = row[0]  # Assuming the sentence is in the first column\n",
        "\n",
        "        # Process the sentence with spaCy\n",
        "        doc = nlp(sentence)\n",
        "\n",
        "        # Extract named entities and their labels\n",
        "        entities = []\n",
        "        for ent in doc.ents:\n",
        "            entities.append((ent.start_char, ent.end_char, ent.label_))\n",
        "\n",
        "        # Create a training example dictionary\n",
        "        example = {\"text\": sentence, \"entities\": entities}\n",
        "\n",
        "        # Append the example to the training data\n",
        "        training_data.append(example)\n",
        "\n",
        "# Save the training data as a JSON file\n",
        "with open('training_data.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(training_data, json_file, ensure_ascii=False)\n",
        "\n",
        "# Now, 'training_data.json' is in the desired format for fine-tuning the spaCy model\n"
      ],
      "metadata": {
        "id": "jPdQVUFPn1-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "from spacy.training.example import Example\n",
        "\n",
        "# Create a blank spaCy model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define and add the NER component to the pipeline\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Load your training data in JSON format\n",
        "with open('training_data.json', 'r', encoding='utf-8') as json_file:\n",
        "    training_data = json.load(json_file)\n",
        "\n",
        "# Define function to convert JSON format data to spaCy Example\n",
        "def convert_to_example(data):\n",
        "    text = data[\"text\"]\n",
        "    entities = data[\"entities\"]\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, {\"entities\": entities})\n",
        "    return example\n",
        "\n",
        "# Convert JSON format training data to spaCy Examples\n",
        "examples = [convert_to_example(data) for data in training_data]\n",
        "\n",
        "# Initialize random weights for NER component\n",
        "ner.initialize(lambda: examples)\n",
        "\n",
        "# Fine-tune the NER model using the annotated data\n",
        "for epoch in range(30):  # Adjust the number of epochs as needed\n",
        "    random.shuffle(examples)\n",
        "    losses = {}\n",
        "\n",
        "    for batch in spacy.util.minibatch(examples, size=2):\n",
        "        nlp.update(batch, drop=0.5, losses=losses)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Losses: {losses}\")\n",
        "\n",
        "# Save the fine-tuned NER model\n",
        "nlp.to_disk(\"fine_tuned_ner_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "s2z1v8pCpoEi",
        "outputId": "701617dc-1747-4e43-cfad-3b05f5339b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-2e3a986b3668>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Convert JSON format training data to spaCy Examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert_to_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Initialize random weights for NER component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-2e3a986b3668>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Convert JSON format training data to spaCy Examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert_to_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Initialize random weights for NER component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-2e3a986b3668>\u001b[0m in \u001b[0;36mconvert_to_example\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/training/example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.Example.from_dict\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/training/example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.annotations_to_doc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/training/example.pyx\u001b[0m in \u001b[0;36mspacy.training.example._add_entities_to_doc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E973] Unexpected type for NER data"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import csv\n",
        "import json\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize the Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define patterns for matching names, numbers, addresses, and passwords\n",
        "name_pattern = [{\"POS\": {\"IN\": [\"PROPN\"]}}, {\"POS\": {\"IN\": [\"PROPN\"]}}]\n",
        "number_pattern = [{\"IS_DIGIT\": True}]\n",
        "address_pattern = [\n",
        "    {\"LIKE_NUM\": True, \"OP\": \"+\"},  # Match one or more numeric tokens (house numbers)\n",
        "    {\"IS_ALPHA\": True, \"OP\": \"+\"},  # Match one or more alphabetic tokens (street names)\n",
        "    {\"IS_SPACE\": True, \"OP\": \"?\"},  # Match zero or one space token (optional)\n",
        "    {\"LIKE_NUM\": True, \"OP\": \"?\"},  # Match zero or one numeric token (ZIP codes or additional info)\n",
        "]\n",
        "\n",
        "password_pattern = [{\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"IS_DIGIT\": True, \"OP\": \"+\"}, {\"LENGTH\": {\">=\": 8}}]\n",
        "\n",
        "# Add the patterns to the Matcher with custom labels\n",
        "matcher.add(\"NAME\", [name_pattern])\n",
        "matcher.add(\"NUMBER\", [number_pattern])\n",
        "matcher.add(\"ADDRESS\", [address_pattern])\n",
        "matcher.add(\"PASSWORD\", [password_pattern])\n",
        "\n",
        "\n",
        "training_data = []\n",
        "\n",
        "# Read your dataset, assuming it's in a CSV file with one row and no headers\n",
        "with open('redaction_train_set.csv', 'r') as csv_file:\n",
        "    # Read the single row from the CSV file\n",
        "    reader = csv.reader(csv_file)\n",
        "    row = next(reader)\n",
        "\n",
        "    # Combine the sentences into one text\n",
        "    text = ' '.join(row)\n",
        "\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Get matches and their spans\n",
        "    matches = matcher(doc)\n",
        "    entities = []\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        label = nlp.vocab.strings[match_id]  # Get the label from the Matcher\n",
        "        entities.append({\"start\": start, \"end\": end, \"label\": label})\n",
        "\n",
        "    # Create a training example dictionary\n",
        "    example = {\"text\": text, \"entities\": entities}\n",
        "\n",
        "    # Append the example to the training data list\n",
        "    training_data.append(example)\n",
        "\n",
        "# Save the training data as a JSON file in the required format for spaCy training\n",
        "with open('training_data.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(training_data, json_file, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "UVwjkReHOp79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Load a blank spaCy model with only the NER component\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Load the training data from the JSON file\n",
        "with open(\"/content/training_data.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
        "    training_data = json.load(json_file)\n",
        "\n",
        "# Add labels for NER\n",
        "for _, annotations in training_data:\n",
        "    for ent in annotations.get(\"entities\"):\n",
        "        ner.add_label(ent[\"label\"])\n",
        "\n",
        "# Initialize the training loop\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "# Training data in the format (text, {\"entities\": [(start, end, label), ...]})\n",
        "for _ in range(30):  # You can adjust the number of epochs\n",
        "    random.shuffle(training_data)\n",
        "    losses = {}\n",
        "\n",
        "    # Batch the examples and iterate over them\n",
        "    for batch in spacy.util.minibatch(training_data, size=2):\n",
        "        texts, annotations = zip(*batch)\n",
        "        nlp.update(texts, annotations, drop=0.5, losses=losses)\n",
        "\n",
        "    print(\"Epoch {}, Losses: {}\".format(_, losses))\n",
        "\n",
        "# Save the trained model\n",
        "nlp.to_disk(\"custom_ner_model\")\n",
        "\n",
        "# Load the trained model for later use\n",
        "# custom_nlp = spacy.load(\"custom_ner_model\")\n",
        "\n",
        "# Test the model on new text\n",
        "# doc = custom_nlp(\"Your test sentence goes here.\")\n",
        "# for ent in doc.ents:\n",
        "#     print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "id": "fEqpUHRSpyuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "4e8909b0-4518-4df0-cc62-c85322220c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-584d14b0fed3>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Add labels for NER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Load a blank spaCy model with only the NER component\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Load the training data from the JSON file\n",
        "with open(\"/content/training_data.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
        "    training_data = json.load(json_file)\n",
        "\n",
        "# Add labels for NER\n",
        "for example in training_data:\n",
        "    text = example[\"content\"]\n",
        "    entities = example[\"entities\"]\n",
        "    for ent in entities:\n",
        "        ner.add_label(ent[\"label\"])\n",
        "\n",
        "# Initialize the training loop\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "# Training data in the format (text, {\"entities\": [(start, end, label), ...]})\n",
        "for _ in range(30):  # You can adjust the number of epochs\n",
        "    random.shuffle(training_data)\n",
        "    losses = {}\n",
        "\n",
        "    # Batch the examples and iterate over them\n",
        "    for batch in spacy.util.minibatch(training_data, size=2):\n",
        "        texts = [example[\"content\"] for example in batch]\n",
        "        annotations = [example[\"entities\"] for example in batch]\n",
        "        nlp.update(texts, annotations, drop=0.5, losses=losses)\n",
        "\n",
        "    print(\"Epoch {}, Losses: {}\".format(_, losses))\n",
        "\n",
        "# Save the trained model\n",
        "nlp.to_disk(\"custom_ner_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "dEo6sfbZOhXS",
        "outputId": "d2789bc3-9dca-40f7-cbba-19832895b4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-d6a776cf9f6d>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}, Losses: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \"\"\"\n\u001b[1;32m   1163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE989\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E989] `nlp.update()` was called with two positional arguments. This may be due to a backwards-incompatible change to the format of the training data in spaCy 3.0 onwards. The 'update' function should now be called with a batch of Example objects, instead of `(text, annotation)` tuples. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "import random\n",
        "from spacy.training.example import Example\n",
        "\n",
        "# Create a blank spaCy model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define and add the NER component to the pipeline\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Load your training data in JSON format\n",
        "with open('training_data.json', 'r', encoding='utf-8') as json_file:\n",
        "    training_data = json.load(json_file)\n",
        "\n",
        "# Convert JSON format training data to spaCy Examples\n",
        "examples = []\n",
        "for data in training_data:\n",
        "    text = data[\"text\"]\n",
        "    entities = data[\"entities\"]\n",
        "    example = Example.from_dict(nlp.make_doc(text), {\"entities\": entities})\n",
        "    examples.append(example)\n",
        "\n",
        "# Initialize random weights for NER component\n",
        "ner.initialize(lambda: examples)\n",
        "\n",
        "# Fine-tune the NER model using the annotated data\n",
        "for epoch in range(30):  # Adjust the number of epochs as needed\n",
        "    random.shuffle(examples)\n",
        "    losses = {}\n",
        "\n",
        "    for batch in spacy.util.minibatch(examples, size=2):\n",
        "        nlp.update(batch, drop=0.5, losses=losses)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Losses: {losses}\")\n",
        "\n",
        "# Save the fine-tuned NER model\n",
        "nlp.to_disk(\"fine_tuned_ner_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFg_D8cuR7Xx",
        "outputId": "98a565a7-e1e9-4fe5-9259-4c6d99851d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Hi, my name is Emma Thompson, and I have forgotten...\" with entities \"[[5, 7, 'NAME']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Losses: {'ner': 11.5}\n",
            "Epoch 2, Losses: {'ner': 9.644946932792664}\n",
            "Epoch 3, Losses: {'ner': 7.815921425819397}\n",
            "Epoch 4, Losses: {'ner': 6.534358471632004}\n",
            "Epoch 5, Losses: {'ner': 5.215380027890205}\n",
            "Epoch 6, Losses: {'ner': 4.235481485724449}\n",
            "Epoch 7, Losses: {'ner': 3.1707327142357826}\n",
            "Epoch 8, Losses: {'ner': 2.152984391897917}\n",
            "Epoch 9, Losses: {'ner': 1.3347723241895437}\n",
            "Epoch 10, Losses: {'ner': 1.2697494034655392}\n",
            "Epoch 11, Losses: {'ner': 0.694668014999479}\n",
            "Epoch 12, Losses: {'ner': 0.4506856445223093}\n",
            "Epoch 13, Losses: {'ner': 0.42553007462993264}\n",
            "Epoch 14, Losses: {'ner': 0.11096235444711056}\n",
            "Epoch 15, Losses: {'ner': 0.11830819259193959}\n",
            "Epoch 16, Losses: {'ner': 0.17797493515718088}\n",
            "Epoch 17, Losses: {'ner': 0.08254448256775504}\n",
            "Epoch 18, Losses: {'ner': 0.01038091994610113}\n",
            "Epoch 19, Losses: {'ner': 0.0032358982315372486}\n",
            "Epoch 20, Losses: {'ner': 0.0048476604875205}\n",
            "Epoch 21, Losses: {'ner': 0.0003913737293457231}\n",
            "Epoch 22, Losses: {'ner': 0.009173259779036158}\n",
            "Epoch 23, Losses: {'ner': 0.0017545475121437393}\n",
            "Epoch 24, Losses: {'ner': 1.717653465420528e-05}\n",
            "Epoch 25, Losses: {'ner': 6.364983468725664e-05}\n",
            "Epoch 26, Losses: {'ner': 1.7763739167762221e-06}\n",
            "Epoch 27, Losses: {'ner': 0.00019515027633622512}\n",
            "Epoch 28, Losses: {'ner': 7.555326377388784e-06}\n",
            "Epoch 29, Losses: {'ner': 7.650139736020203e-07}\n",
            "Epoch 30, Losses: {'ner': 3.392756252401841e-06}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import csv\n",
        "import json\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize the Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define patterns for matching names, numbers, addresses, and passwords\n",
        "name_pattern = [{\"POS\": {\"IN\": [\"PROPN\"]}}, {\"POS\": {\"IN\": [\"PROPN\"]}}]\n",
        "number_pattern = [{\"IS_DIGIT\": True}]\n",
        "address_pattern = [\n",
        "    {\"LIKE_NUM\": True, \"OP\": \"+\"},  # Match one or more numeric tokens (house numbers)\n",
        "    {\"IS_ALPHA\": True, \"OP\": \"+\"},  # Match one or more alphabetic tokens (street names)\n",
        "    {\"IS_SPACE\": True, \"OP\": \"?\"},  # Match zero or one space token (optional)\n",
        "    {\"LIKE_NUM\": True, \"OP\": \"?\"},  # Match zero or one numeric token (ZIP codes or additional info)\n",
        "]\n",
        "\n",
        "password_pattern = [{\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"IS_DIGIT\": True, \"OP\": \"+\"}, {\"LENGTH\": {\">=\": 8}}]\n",
        "\n",
        "# Add the patterns to the Matcher with custom labels\n",
        "matcher.add(\"NAME\", [name_pattern])\n",
        "matcher.add(\"NUMBER\", [number_pattern])\n",
        "matcher.add(\"ADDRESS\", [address_pattern])\n",
        "matcher.add(\"PASSWORD\", [password_pattern])\n",
        "\n",
        "training_data = []\n",
        "\n",
        "# Read your dataset, assuming it's in a CSV file with one row and no headers\n",
        "with open('redaction_train_set.csv', 'r') as csv_file:\n",
        "    # Read the single row from the CSV file\n",
        "    reader = csv.reader(csv_file)\n",
        "    row = next(reader)\n",
        "\n",
        "    # Combine the sentences into one text\n",
        "    text = ' '.join(row)\n",
        "\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Get matches and their spans\n",
        "    matches = matcher(doc)\n",
        "    entities = []\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        label = nlp.vocab.strings[match_id]  # Get the label from the Matcher\n",
        "        entities.append([start, end, label])  # Format as [start, end, label]\n",
        "\n",
        "    # Create a training example dictionary\n",
        "    example = {\"text\": text, \"entities\": entities}\n",
        "\n",
        "    # Append the example to the training data list\n",
        "    training_data.append(example)\n",
        "\n",
        "# Save the training data as a JSON file in the required format for spaCy training\n",
        "with open('training_data.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(training_data, json_file, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "9vHLXklrS1tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "from spacy.training.example import Example\n",
        "from spacy.training import offsets_to_biluo_tags\n",
        "\n",
        "# Load your spaCy model and NER component\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "# Load your training data in JSON format\n",
        "with open('training_data.json', 'r', encoding='utf-8') as json_file:\n",
        "    training_data = json.load(json_file)\n",
        "\n",
        "# Function to check and correct entity offsets\n",
        "def align_entities(text, entities):\n",
        "    doc = nlp.make_doc(text)\n",
        "    corrected_entities = []\n",
        "\n",
        "    tags = offsets_to_biluo_tags(doc, entities)\n",
        "\n",
        "    if '-' in tags:\n",
        "        print(f\"Misaligned entities in text: {text}\")\n",
        "        print(f\"Tags: {tags}\")\n",
        "        # Manually correct entity offsets as needed\n",
        "        # Example: entities[0][\"start\"] = new_start_offset\n",
        "        #          entities[0][\"end\"] = new_end_offset\n",
        "        #          Repeat for other misaligned entities\n",
        "\n",
        "    for entity in entities:\n",
        "        corrected_entities.append({\n",
        "            \"start\": entity[\"start\"],\n",
        "            \"end\": entity[\"end\"],\n",
        "            \"label\": entity[\"label\"]\n",
        "        })\n",
        "\n",
        "    return corrected_entities\n",
        "\n",
        "# Correct and convert JSON format training data to spaCy Examples\n",
        "examples = []\n",
        "\n",
        "for data in training_data:\n",
        "    text = data[\"text\"]\n",
        "    entities = align_entities(text, data[\"entities\"])\n",
        "    example = Example.from_dict(nlp.make_doc(text), {\"entities\": entities})\n",
        "    examples.append(example)\n",
        "\n",
        "# Initialize random weights for NER component\n",
        "ner.initialize(lambda: examples)\n",
        "\n",
        "# Fine-tune the NER model using the corrected training data\n",
        "for epoch in range(30):  # Adjust the number of epochs as needed\n",
        "    random.shuffle(examples)\n",
        "    losses = {}\n",
        "\n",
        "    for batch in spacy.util.minibatch(examples, size=2):\n",
        "        nlp.update(batch, drop=0.5, losses=losses)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Losses: {losses}\")\n",
        "\n",
        "# Save the fine-tuned NER model\n",
        "nlp.to_disk(\"fine_tuned_ner_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "XzIPecJ7TImt",
        "outputId": "ecff2c5c-95f9-4b6f-d394-9d509db98906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misaligned entities in text: Hi, my name is Emma Thompson, and I have forgotten the password for my email account. It's emmathompson95@gmail.com.\n",
            "Tags: ['O', 'O', '-', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-b8ec60b503d5>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-b8ec60b503d5>\u001b[0m in \u001b[0;36malign_entities\u001b[0;34m(text, entities)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         corrected_entities.append({\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import csv\n",
        "import json\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize the Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define patterns for matching names, numbers, addresses, and passwords\n",
        "name_pattern = [{\"POS\": {\"IN\": [\"PROPN\"]}}, {\"POS\": {\"IN\": [\"PROPN\"]}}]\n",
        "number_pattern = [{\"IS_DIGIT\": True}]\n",
        "address_pattern = [\n",
        "    {\"LIKE_NUM\": True, \"OP\": \"+\"},  # Match one or more numeric tokens (house numbers)\n",
        "    {\"IS_ALPHA\": True, \"OP\": \"+\"},  # Match one or more alphabetic tokens (street names)\n",
        "    {\"IS_SPACE\": True, \"OP\": \"?\"},  # Match zero or one space token (optional)\n",
        "    {\"LIKE_NUM\": True, \"OP\": \"?\"},  # Match zero or one numeric token (ZIP codes or additional info)\n",
        "]\n",
        "\n",
        "password_pattern = [{\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"IS_DIGIT\": True, \"OP\": \"+\"}, {\"LENGTH\": {\">=\": 8}}]\n",
        "\n",
        "# Add the patterns to the Matcher with custom labels\n",
        "matcher.add(\"NAME\", [name_pattern])\n",
        "matcher.add(\"NUMBER\", [number_pattern])\n",
        "matcher.add(\"ADDRESS\", [address_pattern])\n",
        "matcher.add(\"PASSWORD\", [password_pattern])\n",
        "\n",
        "training_data = []\n",
        "\n",
        "# Read your dataset, assuming it's in a CSV file with one row and no headers\n",
        "with open('redaction_train_set.csv', 'r') as csv_file:\n",
        "    # Read the single row from the CSV file\n",
        "    reader = csv.reader(csv_file)\n",
        "    row = next(reader)\n",
        "\n",
        "    # Combine the sentences into one text\n",
        "    text = ' '.join(row)\n",
        "\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Get matches and their spans\n",
        "    matches = matcher(doc)\n",
        "    entities = []\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        label = nlp.vocab.strings[match_id]  # Get the label from the Matcher\n",
        "        entities.append((start, end, label))  # Format as (start, end, label)\n",
        "\n",
        "    # Create a training example dictionary\n",
        "    example = {\"content\": text, \"entities\": entities}  # Use \"content\" instead of \"text\"\n",
        "\n",
        "    # Append the example to the training data list\n",
        "    training_data.append(example)\n",
        "\n",
        "# Save the training data as a JSON file in the required format for spaCy training\n",
        "with open('training_data.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(training_data, json_file, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "lMSqFR6OToqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "from spacy.training.example import Example\n",
        "from spacy.training import offsets_to_biluo_tags\n",
        "\n",
        "# Load your spaCy model and NER component\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "# Load your training data in JSON format\n",
        "with open('training_data.json', 'r', encoding='utf-8') as json_file:\n",
        "    training_data = json.load(json_file)\n",
        "\n",
        "# Function to check and correct entity offsets\n",
        "def align_entities(text, entities):\n",
        "    doc = nlp.make_doc(text)\n",
        "    corrected_entities = []\n",
        "\n",
        "    tags = offsets_to_biluo_tags(doc, entities)\n",
        "\n",
        "    if '-' in tags:\n",
        "        print(f\"Misaligned entities in text: {text}\")\n",
        "        print(f\"Tags: {tags}\")\n",
        "        # Manually correct entity offsets as needed\n",
        "        # Example: entities[0][\"start\"] = new_start_offset\n",
        "        #          entities[0][\"end\"] = new_end_offset\n",
        "        #          Repeat for other misaligned entities\n",
        "\n",
        "    for entity in entities:\n",
        "        corrected_entities.append({\n",
        "            \"start\": entity[\"start\"],\n",
        "            \"end\": entity[\"end\"],\n",
        "            \"label\": entity[\"label\"]\n",
        "        })\n",
        "\n",
        "    return corrected_entities\n",
        "\n",
        "# Correct and convert JSON format training data to spaCy Examples\n",
        "examples = []\n",
        "\n",
        "for data in training_data:\n",
        "    text = data[\"text\"]\n",
        "    entities = align_entities(text, data[\"entities\"])\n",
        "    example = Example.from_dict(nlp.make_doc(text), {\"entities\": entities})\n",
        "    examples.append(example)\n",
        "\n",
        "# Initialize random weights for NER component\n",
        "ner.initialize(lambda: examples)\n",
        "\n",
        "# Fine-tune the NER model using the corrected training data\n",
        "for epoch in range(30):  # Adjust the number of epochs as needed\n",
        "    random.shuffle(examples)\n",
        "    losses = {}\n",
        "\n",
        "    for batch in spacy.util.minibatch(examples, size=2):\n",
        "        nlp.update(batch, drop=0.5, losses=losses)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Losses: {losses}\")\n",
        "\n",
        "# Save the fine-tuned NER model\n",
        "nlp.to_disk(\"fine_tuned_ner_model1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "yoh3JC9gUAqf",
        "outputId": "30e9aa10-6b17-461c-e197-98f6518538e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-d0bca1ee60df>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "from spacy.training.example import Example\n",
        "\n",
        "# Create a blank spaCy model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define and add the NER component to the pipeline\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Load your training data in JSON format\n",
        "with open('training_data.json', 'r', encoding='utf-8') as json_file:\n",
        "    training_data = json.load(json_file)\n",
        "\n",
        "# Convert JSON format training data to spaCy Examples\n",
        "examples = []\n",
        "for data in training_data:\n",
        "    text = data[\"content\"]\n",
        "    entities = data[\"entities\"]\n",
        "    example = Example.from_dict(nlp.make_doc(text), {\"entities\": entities})\n",
        "    examples.append(example)\n",
        "\n",
        "# Initialize random weights for NER component\n",
        "ner.initialize(lambda: examples)\n",
        "\n",
        "# Fine-tune the NER model using the annotated data\n",
        "for epoch in range(30):  # Adjust the number of epochs as needed\n",
        "    random.shuffle(examples)\n",
        "    losses = {}\n",
        "\n",
        "    for batch in spacy.util.minibatch(examples, size=2):\n",
        "        nlp.update(batch, drop=0.5, losses=losses)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Losses: {losses}\")\n",
        "\n",
        "# Save the fine-tuned NER model\n",
        "nlp.to_disk(\"fine_tuned_ner_model1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1H0HTtYUKtF",
        "outputId": "ee8a5638-6bb2-4697-9219-78a70334a1cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Losses: {'ner': 11.5}\n",
            "Epoch 2, Losses: {'ner': 9.480030685663223}\n",
            "Epoch 3, Losses: {'ner': 7.643797039985657}\n",
            "Epoch 4, Losses: {'ner': 6.371086210012436}\n",
            "Epoch 5, Losses: {'ner': 4.585914269089699}\n",
            "Epoch 6, Losses: {'ner': 3.5126496627926826}\n",
            "Epoch 7, Losses: {'ner': 2.771520346403122}\n",
            "Epoch 8, Losses: {'ner': 2.029453467577696}\n",
            "Epoch 9, Losses: {'ner': 1.1702608931809664}\n",
            "Epoch 10, Losses: {'ner': 0.9778577508404851}\n",
            "Epoch 11, Losses: {'ner': 0.6007604662445374}\n",
            "Epoch 12, Losses: {'ner': 0.3207415898796171}\n",
            "Epoch 13, Losses: {'ner': 0.15849372651427984}\n",
            "Epoch 14, Losses: {'ner': 0.17070488262106664}\n",
            "Epoch 15, Losses: {'ner': 0.08131436250369006}\n",
            "Epoch 16, Losses: {'ner': 0.0244478685381182}\n",
            "Epoch 17, Losses: {'ner': 0.022496627872442332}\n",
            "Epoch 18, Losses: {'ner': 0.004896798854133522}\n",
            "Epoch 19, Losses: {'ner': 0.006713592645041899}\n",
            "Epoch 20, Losses: {'ner': 0.0027069021497823087}\n",
            "Epoch 21, Losses: {'ner': 0.007122154161097072}\n",
            "Epoch 22, Losses: {'ner': 0.0003775386222590882}\n",
            "Epoch 23, Losses: {'ner': 0.00036620512364755886}\n",
            "Epoch 24, Losses: {'ner': 4.679309190504244e-05}\n",
            "Epoch 25, Losses: {'ner': 6.178412738670758e-05}\n",
            "Epoch 26, Losses: {'ner': 5.1368887547606793e-05}\n",
            "Epoch 27, Losses: {'ner': 3.2220524343973093e-06}\n",
            "Epoch 28, Losses: {'ner': 2.3526775797428185e-05}\n",
            "Epoch 29, Losses: {'ner': 4.5443816974041496e-07}\n",
            "Epoch 30, Losses: {'ner': 6.008458981991425e-09}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sqzd0paPUWwf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}